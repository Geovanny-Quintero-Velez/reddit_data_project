id,title,selftext,score,num_comments,author,created_utc,url,upvote_ratio,over_18,edited,spoiler,stickied
1kk0xxg,Last 2 months I have been humbled by the data engineering landscape,"Hello All,

For the past 6 years I have been working in the data analyst and data engineer role (My title is Senior Data Analyst ). I have been working with Snowflake writing stored procedures, spark using databricks, ADF for orchestration, SQL server, power BI & Tableau dashboards. All the data processing has been either monthly or quarterly. I was always under the impression that I was going to be quite employable when I try to switch at some point.

But the past few months have taught me that there aren't many data analyst openings and the field doesn't pay squat and is mostly for freshers and the data engineering that I have been doing isn't really actual data engineering.

All the openings I see require knowledge of Kafka, docker, kubernetes, microservices, airflow, mlops, API integration, CI/CD etc. This has left me stunned at the very least. I never knew that most of the companies required such a diverse set of skills and data engineering was more of SWE rather than what I have been doing. Seriously not sure what to think of the scenario I am in.",125,44,skatez101,2025-05-11 13:23:56,https://www.reddit.com/r/dataengineering/comments/1kk0xxg/last_2_months_i_have_been_humbled_by_the_data/,0,False,False,False,False
1kjo6uj,What are your ETL data cleaning/standardisation rules?,"As the title says. 

We're in the process of rearchitecting our ETL pipeline design (for a multitude of reasons), and we want a step after ingestion and contract validation where we perform a light level of standardisation so data is more consistent and reusable. For context, we're a low data maturity organisation and there is little-to-no DQ governance over applications, so it's on us to ensure the data we use is fit for use.

These are our current thinking on rules; what do y'all do out there for yours?

* UTF-8 and parquet
* ISO-8601 datetime format
* NFC string normalisation (one of our country's languages uses macrons)
* Remove control characters - Unicode category ""C""
* *Remove invalid UTF-8 characters?? e.g. str.encode/decode process*
* Trim leading/trailing whitespace

(Deduplication is currently being debated as to whether it's a contract violation or something we handle)",67,25,Nightwyrm,2025-05-11 00:13:41,https://www.reddit.com/r/dataengineering/comments/1kjo6uj/what_are_your_etl_data_cleaningstandardisation/,0,False,False,False,False
1kk5zup,"Why is ""Sort Merge Join"" is preferred over ""Shuffle Hash Join"" in Spark?","Hi all! 

I am trying to upgrade my Spark skills (mainly using it as a user with little optimization) and some questions came to mind. I am reading everywhere that ""Sorted Merge Join"" is preferred over ""Shuffle Hash Join"" because:

1. Avoids building a hash table.
2. Allows to spill to disk.
3. It is more scalable (as doesn't need to store the hashmap into memory). Which makes sense.

Can any of you be kind enough to explain:

* How sorting both tables (O(n log n)) is faster than building a hash table O(n)?
* Why can't a hash table be spilled to disk (even on its own format)?

  
",11,6,DataGhost404,2025-05-11 17:12:49,https://www.reddit.com/r/dataengineering/comments/1kk5zup/why_is_sort_merge_join_is_preferred_over_shuffle/,0,False,False,False,False
1kjsirz,10 Must-Know Queries to Observe Snowflake Performance ‚Äî Part 1,"Hi all ‚Äî I recently wrote a practical guide that walks through 10 SQL queries you can use to **observe Snowflake performance** before diving into any tuning or optimization.

The post includes queries to:

* Identify long-running and expensive queries
* Detect warehouse queuing and disk spillage
* Monitor cache misses and slow task execution
* Spot heavy data scans

These are the queries I personally find most helpful when trying to understand what‚Äôs *really* going on inside Snowflake ‚Äî especially before looking at clustering or tuning pipelines.

Here's the link:  
üëâ [https://medium.com/@arunkumarmadhavannair/10-must-know-queries-to-observe-snowflake-performance-part-1-f927c93a7b04](https://medium.com/@arunkumarmadhavannair/10-must-know-queries-to-observe-snowflake-performance-part-1-f927c93a7b04)

Would love to hear if you use any similar queries or have other suggestions!",8,0,Neat-Resort9968,2025-05-11 04:19:58,https://www.reddit.com/r/dataengineering/comments/1kjsirz/10_mustknow_queries_to_observe_snowflake/,0,False,False,False,False
1kjx0a2,Need help deciding- ML vs DE,"So I got internship offers for both machine learning and data engineering but I‚Äôm not sure which one to pick. I don‚Äôt mind doing either and they both pay the same.

Which one would be better in terms of future jobs opportunities, career advancement, resistance to layoffs, and pay? I don‚Äôt plan on going to grad school.",3,7,Physical-Maximum2763,2025-05-11 09:23:50,https://www.reddit.com/r/dataengineering/comments/1kjx0a2/need_help_deciding_ml_vs_de/,0,False,False,False,False
1kjn1gu,Single shot a streamlit and gradio app into existence,"Hey everyone, wanted to share an experimental tool,¬†[https://v1.slashml.com](https://v1.slashml.com/?utm_source=reddit_streamlit), it can build streamlit, gradio apps and host them with a unique url, from a single prompt.

The frontend is mostly vibe-coded. For the backend and hosting I use a big instance with nested virtualization and spinup a VM with every preview. The url routing is done in nginx.

Would love for you to try it out and any feedback would be appreciated. ",5,3,fazkan,2025-05-10 23:14:23,https://www.reddit.com/r/dataengineering/comments/1kjn1gu/single_shot_a_streamlit_and_gradio_app_into/,0,False,False,False,False
1kjxab3,I built a database of WSL players' performance stats using data scraped from Fbref,"


On one hand, I needed the data as I wanted to analyse the performance of my favourite players in the Women Super League. On the other hand, I'd finished an Introduction To Databases course offered by CS50 and the final project was to build a database.

So killing both birds with one stone, I built the database using data starting from the 2021-22 season and until this current season (2024-25). 

I scrape and clean the data in notebooks, multiple notebooks as there are multiple tables focusing on different aspects of performance e.g. shooting, passing, defending, goalkeeping, pass types etc.

I then create relationships across the tables and then load them into a database I created in Google's BigQuery. 

At first I collected and only used data from previous seasons to set up the database, before updating it with this current season's data. As the current season hasn't ended (actually ended last Saturday), I wanted to be able to handle more recent updates by just rerunning the notebooks without affecting other season's data. That's why the current season is handled in a different folder, and newer seasons will have their own folders too.

I'm a beginner in terms of databases and the methods I use reflect my current understanding.



TLDR: I built a database of Women Super League players using data scraped from Fbref. The data starts from the 2021-22 till this current season. Rerunning the current season's notebooks collects and updates the database with more recent data.",2,0,play_ads,2025-05-11 09:43:48,https://github.com/second-week/women-football-database,0,False,False,False,False
1kk9jxn,Deep research over Google Drive (open source!),"Hey¬†r/dataengineering ¬†community!

We've added Google Drive as a connector in¬†[Morphik](https://morphik.ai/), which is one of the most requested features.

# What is Morphik?

Morphik is an open-source end-to-end RAG stack. It provides both self-hosted and managed options with a python SDK, REST API, and clean UI for queries. The focus is on accurate retrieval without complex pipelines, especially for visually complex or technical documents. We have knowledge graphs, cache augmented generation, and also options to run isolated instances great for air gapped environments.

# Google Drive Connector

You can now connect your Drive documents directly to Morphik, build knowledge graphs from your existing content, and query across your documents with our research agent. This should be helpful for projects requiring reasoning across technical documentation, research papers, or enterprise content.

Disclaimer: still waiting for app approval from google so¬†might¬†be one or two extra clicks to authenticate.

# Links

* Try it out:¬†[https://morphik.ai](https://morphik.ai/)
* GitHub:¬†[https://github.com/morphik-org/morphik-core](https://github.com/morphik-org/morphik-core)¬†(Please give us a ‚≠ê)
* Docs:¬†[https://docs.morphik.ai](https://docs.morphik.ai/)
* Discord:¬†[https://discord.com/invite/BwMtv3Zaju](https://discord.com/invite/BwMtv3Zaju)

We're planning to add more connectors soon. What sources would be most useful for your projects? Any feedback/questions welcome!

# ",2,1,yes-no-maybe_idk,2025-05-11 19:46:41,https://www.reddit.com/r/dataengineering/comments/1kk9jxn/deep_research_over_google_drive_open_source/,0,False,False,False,False
1kjxv6l,Data governance tools,"What are the data governance tools that can enable the teams to see what kind of data assets live, who and all are using a particular type of data? Suggestions are welcomed anything related to AWS or open source tools",2,1,arunrajan96,2025-05-11 10:23:19,https://www.reddit.com/r/dataengineering/comments/1kjxv6l/data_governance_tools/,0,False,False,False,False
1kjxa2i,New project advise,"We are starting on a project which involves salesforce api, transformations and redshift db. Below are exact specs regarding the project.

1) one time read and save historical data to redshift (3 million records, data size - 6 GB)

2) Read incremental data on a daily basis from salesforce using api (to query 100000 records per batch)

3) perform data transformations using data quality rules

4) saving final data by implementing data merging using upserts to redshift table

5) Handling logs to handle exceptions which arise during processing.

Would like to know your inputs and the approach that should be followed to develop a workflow using aws stack and helps to get an optimum solution with minimum costs ? I am planning to use glue with redshift and eventbridge.",2,1,First-Possible-1338,2025-05-11 09:43:20,https://www.reddit.com/r/dataengineering/comments/1kjxa2i/new_project_advise/,1,False,False,False,False
1kkaz9g,Iceberg or delta lake,"Which format is better iceberg or delta lake when you want to query from both snowflake and databricks ?? 

",2,3,Own_Art1586,2025-05-11 20:48:38,https://www.reddit.com/r/dataengineering/comments/1kkaz9g/iceberg_or_delta_lake/,1,False,False,False,False
1kkayma,Does anyone have .ova file containing Hadoop and Spark?,"Hi, 

I'm looking for an .ova file containing Hadoop and Spark. The ones available on the internet seem to be missing the [start.dhs.sh](http://start.dhs.sh), etc commands.

I have tried manually downloading the software, but couldn't get past the .bashrc issue, and it would not recognize the above commands. Anything that works will be great. I'm only practising, and versions don't matter.

Thank you.",2,0,sacred__nelumbo,2025-05-11 20:47:50,https://www.reddit.com/r/dataengineering/comments/1kkayma/does_anyone_have_ova_file_containing_hadoop_and/,1,False,False,False,False
1kkaode,Need help building this Project,"I recently had an meeting for a data-related internship. Just a bit about my background: I have over a year of experience working as a backend developer using Django. The company I interviewed with is a startup based in Europe, and they‚Äôre working on building their own LLM using synthetic data.

I had the meeting with one of the cofounders. I applied for a data engineering role, since I‚Äôve done some projects in that area. But the role might change a bit ‚Äî from what I understood, a big part of the work is around data generation. He also mentioned that he has a project in mind for me, which may involve LLMs and fine-tuning which I need to finish in order to finally get the contract for the Job.

I‚Äôve built end-to-end pipelines before and have a basic understanding of libraries like pandas, numpy, and some machine learning models like classification and regression. Still, I‚Äôm feeling unsure and doubting myself, especially since there‚Äôs not been a detailed discussion about the project yet. Just knowing that it may involve LLMs and ML/DL is making me nervous.Because my experiences are purely Data Engineering related and Backed development.

I‚Äôd really appreciate some guidance on :

‚Äî how should I approach this kind of project  once assigned that requires knowledge of LLMs and ML knowing my background, which I don‚Äôt have in a good way. 

Would really appreciate your efforts if you could guide me on this.
",1,1,___Nik_,2025-05-11 20:35:16,https://www.reddit.com/r/dataengineering/comments/1kkaode/need_help_building_this_project/,0,False,False,False,False
1kk7y09,Need advice on freelancing,"I am in the DE field since last 4.5 years and have worked on few data projects. I want to start freelancing to explore new opportunities and get wide array of skillsets, which is not always possible to gain from the day job.

I need help to understand following things
1. What skillsets are in demand for freelancing that I could learn?
2. How many gigs are available for the grab in the market?
3. How do I land some beginner projects( I'm ready to compromise on the fees)?
4. How do i build the strong connections in DE so that I can build trust and create personal brand?


I know this is like everything about freelancing in DE but any help will be appreciated.

Thanks!",0,2,deadprisoner,2025-05-11 18:37:13,https://www.reddit.com/r/dataengineering/comments/1kk7y09/need_advice_on_freelancing/,0,False,False,False,False
1kka3r2,Convert any data format to any data format,"‚ÄúSpent last night vibe coding [https://anytoany.ai](https://anytoany.ai) ‚Äî convert CSV, JSON, XML, YAML instantly. Paid users get 100 conversions. Clean, fast, simple. Soft launching today. Feedback welcome! ‚ù§Ô∏è‚Äù



",1,1,Popular-Stay-2637,2025-05-11 20:10:14,https://www.reddit.com/r/dataengineering/comments/1kka3r2/convert_any_data_format_to_any_data_format/,0,False,False,False,False
1kjqn0b,What data platform pain are you trying to solve most?,"Which pain is most relevant to you? Please elaborate in comments.

[View Poll](https://www.reddit.com/poll/1kjqn0b)",0,4,AMDataLake,2025-05-11 02:29:43,https://www.reddit.com/r/dataengineering/comments/1kjqn0b/what_data_platform_pain_are_you_trying_to_solve/,0,False,False,False,False
1kjuwnl,Do People Actually Code as They Climb the Career Ladder?,"Do People Actually Code as They Climb the Career Ladder?

When I first started my career in tech more than a decade ago, I had this naive assumption that coding was something you did forever, no matter where you were on the career ladder.

But as I‚Äôve climbed higher up the ladder, I‚Äôve realized it‚Äôs not quite that simple.

The truth is, your role evolves. And while coding remains a critical part of many tech careers, its prominence shifts depending on the level you‚Äôre at. Here‚Äôs what I‚Äôve learned along the way --

üî∏In the beginning, coding is everything. You‚Äôre building your foundation - learning programming languages, frameworks, and debugging skills.
üî∏As you grow into mid-level roles, things start to change. You‚Äôre no longer just executing tasks but you‚Äôre leading small teams, mentoring juniors, and contributing to architectural decisions. Your value isn‚Äôt just in writing code but also in understanding *why* certain solutions are chosen over others.
üî∏By the time you reach senior or lead positions, your focus has likely shifted even further away from daily coding. Instead, you‚Äôre setting technical direction, defining best practices, and ensuring alignment across teams. Yes, you might still dive into code occasionally, but it‚Äôs usually to unblock critical issues or set an example.
üî∏If you move into management, or even executive roles, your relationship with coding will transform again. At this point, your primary responsibility is people and strategy. Writing code becomes rare, though having a strong technical background gives you credibility and insight into challenges your team faces.

So‚Ä¶ Do People Actually Code As They Climb?

üî∫Yes, but the amount and type of coding vary greatly depending on your role. For individual contributors (ICs), especially those aiming for principal engineer tracks, coding remains central. For managers or leaders, it becomes more about guiding strategy and enabling others to shine.

To anyone navigating this path, I‚Äôd love to hear your thoughts. How has your relationship with coding changed as you‚Äôve grown in your career?",0,8,arvindspeaks,2025-05-11 06:57:10,https://www.reddit.com/r/dataengineering/comments/1kjuwnl/do_people_actually_code_as_they_climb_the_career/,0,False,False,False,False
